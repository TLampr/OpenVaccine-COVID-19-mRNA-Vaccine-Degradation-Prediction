{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import Counter\nfrom copy import deepcopy\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.utils import shuffle\n\nimport torch\nimport torch.nn as nn\nfrom math import ceil\nfrom torch.utils.data import TensorDataset, RandomSampler, DataLoader\nfrom tqdm import tqdm\nimport math\nfrom torch.autograd import Variable\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IMPORT DATA**","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_json('../input/stanford-covid-vaccine/train.json',lines=True)\ntest_data = pd.read_json('../input/stanford-covid-vaccine/test.json',lines=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SEPARATE THE PUBLIC AND PRIVATE TEST DATASETS BASED ON THE SEQUENCE LENGTH (NOT YET)**","metadata":{}},{"cell_type":"code","source":"# public_test_data = test_data\npublic_test_data = test_data[test_data['seq_length'] == 107]\nprivate_test_data = test_data[test_data['seq_length'] > 107]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GET THE FILTERED TRAINING DATA (NOT YET)**","metadata":{}},{"cell_type":"code","source":"train_data = train_data[train_data['SN_filter'] == 1]\ntrain_bpps_ids = [id for id in train_data['id']]\nprivate_test_bpps_ids = [id for id in private_test_data['id']]\npublic_test_bpps_ids = [id for id in public_test_data['id']]\ntest_bpps_ids = [id for id in public_test_data['id']]\npath = '/kaggle/input/stanford-covid-vaccine/bpps/'\ntrain_bpps = np.array([np.load(path + i + '.npy') for i in train_bpps_ids])\nprivate_test_bpps = np.array([np.load(path + i + '.npy') for i in private_test_bpps_ids])\npublic_test_bpps = np.array([np.load(path + i + '.npy') for i in public_test_bpps_ids])\ntrain_bpps_pad = np.zeros((train_bpps.shape[0], train_bpps.shape[1], 130))\ntrain_bpps_pad[:, :, :107] = train_bpps\npublic_test_bpps_pad = np.zeros((public_test_bpps.shape[0], public_test_bpps.shape[1], 130))\npublic_test_bpps_pad[:, :, :107] = public_test_bpps","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**INITIALIZE FEATURE PREPROCESSING FUNCTIONS**","metadata":{}},{"cell_type":"code","source":"def pair_indices(structure, sequence):\n    stack_base = []\n    stack_index = []\n    seqpairposmap = []\n    seqpairposmapbase = []\n    seqpairpos_dist = []\n    for i in range(len(structure)):\n        if structure[i] == '(':\n            stack_base.append(sequence[i])\n            stack_index.append(i)\n        if structure[i] == ')':\n            pairpos = stack_index.pop()\n            pairpos_base = stack_base.pop()\n            seqpairposmap.append((i, pairpos))\n            pair = (sequence[i], pairpos_base)\n            pair = tuple(sorted(pair))\n            seqpairposmapbase.append(pair)\n    return seqpairposmapbase","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pair_sequence(structure, sequence):\n    stack_index = []\n    pairSequence = []\n    pairMap = []\n    for i in range(len(structure)):\n        if structure[i] == '(':\n            stack_index.append(i)\n        if structure[i] == ')':\n            pairpos = stack_index.pop()\n            pairMap.append((i, pairpos))\n    for i in range(len(sequence)):\n        paired = 'XX'\n        for pairIndex1, pairIndex2 in pairMap:\n            if i == pairIndex1 or i == pairIndex2:\n                paired = ''.join(sorted(sequence[pairIndex1] + sequence[pairIndex2]))\n        pairSequence.append(paired)\n    return pairSequence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GET THE AMOUNT AND TYPES OF PAIRS PER SEQUENCE**","metadata":{}},{"cell_type":"code","source":"public_test_data['pairs'] = public_test_data.apply(lambda x: pair_indices(x.structure, x.sequence), axis=1)\npublic_test_data['pairs'] = public_test_data['pairs'].apply(lambda x: Counter(x))\nprivate_test_data['pairs'] = private_test_data.apply(lambda x: pair_indices(x.structure, x.sequence), axis=1)\nprivate_test_data['pairs'] = private_test_data['pairs'].apply(lambda x: Counter(x))\npublic_test_data = pd.concat([public_test_data.drop(['pairs'], axis=1), public_test_data['pairs'].apply(pd.Series)], axis=1)\ntrain_data['pairs'] = train_data.apply(lambda x: pair_indices(x.structure, x.sequence), axis=1)\ntrain_data['pairs'] = train_data['pairs'].apply(lambda x: Counter(x))\ntrain_data = pd.concat([train_data.drop(['pairs'], axis=1), train_data['pairs'].apply(pd.Series)], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GET THE COUNTS OF EACH BASE PER SEQUENCE**","metadata":{"trusted":true}},{"cell_type":"code","source":"public_test_data['base_counts'] = public_test_data['sequence'].apply(lambda x: Counter(x))\npublic_test_data = pd.concat([public_test_data.drop(['base_counts'], axis=1), public_test_data['base_counts'].apply(pd.Series)], axis=1)\nprivate_test_data['base_counts'] = private_test_data['sequence'].apply(lambda x: Counter(x))\nprivate_test_data = pd.concat([private_test_data.drop(['base_counts'], axis=1), private_test_data['base_counts'].apply(pd.Series)], axis=1)\ntrain_data['base_counts'] = train_data['sequence'].apply(lambda x: Counter(x))\ntrain_data = pd.concat([train_data.drop(['base_counts'], axis=1), train_data['base_counts'].apply(pd.Series)], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GET THE PAIR SEQUENCE**","metadata":{}},{"cell_type":"code","source":"train_data['pair_sequence'] = train_data[['sequence', 'structure']].apply(lambda x: pair_sequence(x.structure, x.sequence), axis=1)\npublic_test_data['pair_sequence'] = public_test_data[['sequence', 'structure']].apply(lambda x: pair_sequence(x.structure, x.sequence), axis=1)\nprivate_test_data['pair_sequence'] = private_test_data[['sequence', 'structure']].apply(lambda x: pair_sequence(x.structure, x.sequence), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**INITIALIZE THE ONE-HOT ENCODER**","metadata":{}},{"cell_type":"code","source":"encoder = OneHotEncoder()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TRAIN THE ONE-HOT ENCODER ON AN EXAMPLE SEQUENCE**","metadata":{}},{"cell_type":"code","source":"sample = train_data['sequence'][0]\nsample = [char for char in sample]\nencoder.fit(np.array(sample).reshape(-1,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**APPLY THE NOW TRAINED ENCODER TO ALL THE SEQUENCES**","metadata":{"trusted":true}},{"cell_type":"code","source":"public_test_data['sequence_encoding'] = public_test_data['sequence'].apply(lambda x: [char for char in x])\npublic_test_data['sequence_encoding']=public_test_data['sequence_encoding'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())\nprivate_test_data['sequence_encoding'] = private_test_data['sequence'].apply(lambda x: [char for char in x])\nprivate_test_data['sequence_encoding']=private_test_data['sequence_encoding'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())\ntrain_data['sequence_encoding'] = train_data['sequence'].apply(lambda x: [char for char in x])\ntrain_data['sequence_encoding']=train_data['sequence_encoding'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TRAIN THE ONE-HOT ENCODER ON AN EXAMPLE PREDICTED LOOP TYPE**","metadata":{"trusted":true}},{"cell_type":"code","source":"encoder_data = train_data['predicted_loop_type'][0] + train_data['predicted_loop_type'][2] + train_data['predicted_loop_type'][5]\nencoder_data = [char for char in encoder_data]\nencoder_data = np.array(encoder_data).reshape(-1,1)\nencoder.fit(encoder_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**APPLY THE NOW TRAINED ENCODER TO ALL THE PREDICTED LOOP TYPES**","metadata":{"trusted":true}},{"cell_type":"code","source":"train_data['predicted_loop_type_encoding'] = train_data['predicted_loop_type'].apply(lambda x: [char for char in x])\ntrain_data['predicted_loop_type_encoding'] = train_data['predicted_loop_type_encoding'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())\npublic_test_data['predicted_loop_type_encoding'] = public_test_data['predicted_loop_type'].apply(lambda x: [char for char in x])\npublic_test_data['predicted_loop_type_encoding'] = public_test_data['predicted_loop_type_encoding'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())\nprivate_test_data['predicted_loop_type_encoding'] = private_test_data['predicted_loop_type'].apply(lambda x: [char for char in x])\nprivate_test_data['predicted_loop_type_encoding'] = private_test_data['predicted_loop_type_encoding'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TRAIN THE ONE-HOT ENCODER ON AN EXAMPLE PAIR SEQUENCE**","metadata":{}},{"cell_type":"code","source":"sample = train_data['pair_sequence'][0]\nsample = np.array(sample).reshape(-1,1)\nencoder.fit(sample)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**APPLY THE NOW TRAINED ENCODER TO ALL PAIR SEQUENCES**","metadata":{}},{"cell_type":"code","source":"train_data['pair_sequence_encoding']=train_data['pair_sequence'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())\npublic_test_data['pair_sequence_encoding']=public_test_data['pair_sequence'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())\nprivate_test_data['pair_sequence_encoding']=private_test_data['pair_sequence'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GET THE DATA IN AN NUMPY ARRAY FORMAT / THE PUBLIC TEST DATA DOES NOT HAVE LABELS (?)**","metadata":{"trusted":true}},{"cell_type":"code","source":"training_array = np.array([np.concatenate(train_data[['sequence_encoding', 'predicted_loop_type_encoding', 'pair_sequence_encoding']].to_numpy()[i], axis=1) for i in range(train_data.shape[0])])\npublic_test_array = np.array([np.concatenate(public_test_data[['sequence_encoding', 'predicted_loop_type_encoding', 'pair_sequence_encoding']].to_numpy()[i], axis=1) for i in range(public_test_data.shape[0])])\nprivate_test_array = np.array([np.concatenate(private_test_data[['sequence_encoding', 'predicted_loop_type_encoding', 'pair_sequence_encoding']].to_numpy()[i], axis=1) for i in range(private_test_data.shape[0])])\ntraining_array_targets = np.array([np.concatenate((np.array(train_data['reactivity'][i]).reshape(1,-1), np.array(train_data['deg_Mg_pH10'][i]).reshape(1,-1), np.array(train_data['deg_Mg_50C'][i]).reshape(1,-1), np.array(train_data['deg_pH10'][i]).reshape(1,-1), np.array(train_data['deg_50C'][i]).reshape(1,-1)), axis=0) for i in train_data.index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CREATE THE OBJECTS OF OUR MODELS**","metadata":{"trusted":true}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, pre_train: bool, **kwargs):\n        super(Encoder, self).__init__()\n        if torch.cuda.is_available():\n            self.dev = \"cuda:0\"\n        else:\n            self.dev = \"cpu\"\n        self.pre_train = pre_train\n        self.input_shape = kwargs[\"input_shape\"]\n        self.n_output = kwargs[\"output_shape\"]\n        self.d_model = kwargs[\"d_model\"]\n        self.nhead = kwargs[\"nhead\"]\n        self.num_layers = kwargs[\"num_layers\"]\n        self.dim_feedforward = kwargs[\"dim_feedforward\"]\n        self.dropout_proba = kwargs[\"dropout_proba\"]\n        self.layer_norm = nn.LayerNorm(self.d_model)\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.nhead ,dim_feedforward=self.dim_feedforward, activation='gelu', dropout=self.dropout_proba)\n        self.fully_connected = nn.Linear(in_features=self.d_model, out_features=self.n_output)\n        self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layer,\n                                             num_layers=self.num_layers, norm=self.layer_norm)\n        self.weighted_sum_1 = nn.Linear(in_features=3 * self.d_model + 32, out_features=self.d_model) # + 32 for the bpps encoding\n        self.weighted_sum_2 = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n        self.bpps_encoding_layer = nn.Linear(in_features=130, out_features=32)\n        self.Embedding_layer = nn.Embedding(self.input_shape, self.d_model)\n        self.vocabulary_indices = np.array(range(self.input_shape)).reshape(1, -1)\n        self.dropout_1 = nn.Dropout(p=self.dropout_proba)\n        self.dropout_2 = nn.Dropout(p=self.dropout_proba)\n        self.ReLU = nn.ReLU()\n        self.Tanh = nn.Tanh()\n        self.positional_encoding=PositionalEncoder(d_model=self.d_model)\n        if pre_train is True:\n            self.mlm_dropout = torch.nn.Dropout2d(p=.1)\n        # else:\n        #     self.mlm_dropout = torch.nn.Dropout2d(p=.0)\n\n    def forward(self, x): # samples, sequence, dim\n        x_indices = self.get_indices(x[:, :, :self.input_shape].cpu())\n        x_bpps = self.bpps_encoding_layer(x[:, :, self.input_shape:].to(self.dev))\n        x_bpps = self.ReLU(x_bpps)\n        x_out = self.Embedding_layer(x_indices.long().to(self.dev)).to(self.dev)\n        x_out = x_out.reshape(x.shape[0], x.shape[1], 3 * self.d_model)\n        if self.pre_train:\n            with torch.no_grad():\n                self.mlm_dropout.train()\n                x_out = self.mlm_dropout(x_out.unsqueeze_(2)).squeeze()\n                x_out_mlm = x_out.detach().clone()\n                x_out_mlm_indices = (x_out_mlm.reshape(-1, 3 * self.d_model).sum(1) == 0).nonzero()\n        x_out = self.ReLU(x_out)\n        x_out = torch.cat((x_out, x_bpps), 2)\n        x_out = self.weighted_sum_1(x_out)\n        # x_out = self.gaussian_noise(x=x_out, std=1e-1)\n        # trial of positional encoding\n        x_out = self.ReLU(x_out)\n        x_out = self.dropout_1(x_out)\n        x_out = self.positional_encoding(x_out)\n        x_out = self.weighted_sum_2(x_out)\n        x_out = self.ReLU(x_out)\n        # x_out = self.dropout_2(x_out)\n        x_out = self.encoder(x_out.transpose(0, 1)) # sequence, samples, dim\n        x_out = self.fully_connected(x_out)\n        if self.pre_train is True:\n            return (x_out, x_out_mlm_indices)\n        return x_out\n    \n    def get_indices(self, x):\n        shape = x.shape\n        x = x.numpy().astype('float')\n        x[x == 0] = np.nan\n        x_values = np.multiply(x, self.vocabulary_indices)\n        return torch.tensor(x_values[~np.isnan(x_values)].reshape(shape[0], shape[1], 3))\n    \n    def gaussian_noise(self, x, std: float):\n        shape = x.shape\n        x += (std**0.5)*torch.randn(shape).to(self.dev)\n        return x\n    \nclass PositionalEncoder(nn.Module):\n    def __init__(self, d_model, max_seq_len=130):\n        super().__init__()\n        self.d_model = d_model\n        \n        # create constant 'pe' matrix with values dependant on \n        # pos and i\n        pe = torch.zeros(max_seq_len, d_model)\n        for pos in range(max_seq_len):\n            for i in range(0, d_model, 2):\n                pe[pos, i] = \\\n                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n                pe[pos, i + 1] = \\\n                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n                \n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n \n    \n    def forward(self, x):\n        # make embeddings relatively larger\n        x = x * math.sqrt(self.d_model)\n        # add constant to embedding\n        seq_len = x.size(1)\n        x = x + Variable(self.pe[:,:seq_len], requires_grad=False)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CREATE THE OBJECTS OF THE LOSS FUNCTIONS**","metadata":{}},{"cell_type":"code","source":"class Pre_train_loss(nn.Module):\n    def _init_(self):\n        super()._init_()\n        \n    def forward(self, yhat, y):   \n        loss = nn.CrossEntropyLoss()\n        yhat, dropout_indices = yhat\n        yhat = yhat.reshape(-1, 15)[dropout_indices].squeeze()\n        y = y[:, :, :15].reshape(-1, 15)[dropout_indices].squeeze()\n        loss_base = loss(yhat[:, :4], torch.argmax(y[:, :4], 1))\n        loss_loop = loss(yhat[:, 4:11], torch.argmax(y[:, 4:11], 1))\n        loss_pair = loss(yhat[:, 11:15], torch.argmax(y[:, 11:15], 1))\n        loss_full = 0.1*loss_base + 0.65*loss_loop + 0.25*loss_pair\n        return loss_full\n\n\nclass Our_Loss(nn.Module):\n    def __init__(self, num_targets: int):\n        super().__init__()\n        self.eps = 1e-6\n        self.num_targets = num_targets\n\n    def forward(self, yhat, y):\n        if isinstance(yhat, tuple) is True: # if we want to pretrain but not with classification\n            yhat, dropout_indices = yhat\n        error = y[:, :, :self.num_targets] - yhat[:y.shape[0], :, :self.num_targets] \n        error = torch.square(error)\n        error = torch.mean(error, dim=0) + self.eps\n        error = torch.sqrt(error)\n        error = torch.mean(error, dim=1)\n        error = torch.mean(error)\n        return error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CREATE THE OBJECT THAT WILL PERFORM THE TRAINING PROCESS**","metadata":{}},{"cell_type":"code","source":"class Optimizer():\n    def __init__(self, model: torch.nn):\n        if torch.cuda.is_available():\n            self.dev = \"cuda:0\"\n        else:\n            self.dev = \"cpu\"\n        self.model = model.to(self.dev)\n\n    def get_optimizer(self, lr: float):\n        return torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5)\n\n    def get_criterion(self, num_targets: int):\n        if num_targets > 0:\n            return Our_Loss(num_targets=num_targets)\n        else: # Pretrain!\n            return Pre_train_loss()\n    \n    def optimize(self, X: torch.Tensor, Y: torch.Tensor, optimizer: torch.optim.Adam, num_targets: int):\n        optimizer.zero_grad()\n        Y_out = self.model(X)\n        criterion = self.get_criterion(num_targets=num_targets)\n        loss = criterion(Y_out, Y.transpose(0, 1).to(self.dev))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.)\n        optimizer.step()\n        return loss.detach().item()\n\n    def evaluate(self, X: torch.Tensor, Y: torch.Tensor, num_targets: int):\n        self.model.eval()\n        with torch.no_grad():\n            Y_out = self.model(X)\n            criterion = self.get_criterion(num_targets=num_targets)\n            loss = criterion(Y_out, Y.transpose(0, 1).to(self.dev))\n        self.model.train()\n        return loss.detach().item()\n    \n    def predict(self, X):\n        self.model.eval()\n        with torch.no_grad():\n            return self.model(X)\n\n    def get_dataloader(self, X: torch.Tensor, Y: torch.Tensor, batch_size: int) -> DataLoader:\n        dataset = TensorDataset(X, Y)\n        sampler = RandomSampler(dataset)\n        return DataLoader(dataset=dataset, sampler=sampler, batch_size=batch_size)\n\n    def fit(self, X: torch.Tensor, Y: torch.Tensor, X_val: torch.Tensor, Y_val: torch.Tensor, epochs: int, lr: float, batch_size: int, patience: int, best_model: bool, num_targets: int):\n        X_train, X_val, Y_train, Y_val = X, X_val, Y, Y_val\n        train_dataset = self.get_dataloader(X=X_train, Y=Y_train, batch_size=batch_size)\n        val_dataset = self.get_dataloader(X=X_val, Y=Y_val, batch_size=batch_size)\n        optimizer = self.get_optimizer(lr=lr)\n        train_loss_history, val_loss_history = [], []\n        old_loss = np.inf\n        counter = 0\n        for epoch in range(epochs):\n            train_batch_loss_history, val_batch_loss_history = [], []\n            for step_num, [X_batch, Y_batch] in enumerate(train_dataset):\n                step_loss = self.optimize(X=X_batch.to(self.dev), Y=Y_batch.to(self.dev), optimizer=optimizer, num_targets=num_targets)\n                train_batch_loss_history.append(step_loss)\n            train_loss_history.append(np.average(train_batch_loss_history))\n            for step_num, [X_batch, Y_batch] in enumerate(val_dataset):\n                val_step_loss = self.evaluate(X=X_batch.to(self.dev), Y=Y_batch.to(self.dev), num_targets=num_targets)\n                val_batch_loss_history.append(val_step_loss)\n            val_loss = np.average(val_batch_loss_history)\n            if old_loss > val_loss:\n                counter = 0\n                old_loss = val_loss.copy()\n                model = deepcopy(self.model)\n            else:\n                counter += 1\n            val_loss_history.append(val_loss)\n            print(f\"train loss: {train_loss_history[-1]} \\tvalidation loss: {val_loss_history[-1]} for epoch {epoch + 1}\")\n            if counter == patience:\n                self.model = deepcopy(model)\n                break\n        if best_model:\n            self.model = deepcopy(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CREATE THE VALIDATION SPLIT FUNCTION**","metadata":{}},{"cell_type":"code","source":"def get_validation_dataset(X: torch.Tensor, Y: torch.Tensor, split_ratio: float):\n    X, Y = shuffle(X, Y)\n    X_train, X_val = X[ceil(split_ratio*len(X)):], X[:ceil(split_ratio*len(X))]\n    Y_train, Y_val = Y[ceil(split_ratio * len(Y)):], Y[:ceil(split_ratio * len(Y))]\n    return X_train, X_val, Y_train, Y_val\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"public_test_bpps.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ADD THE BPPS VECTOR TO EACH TIMESTEP**","metadata":{}},{"cell_type":"code","source":"training_array = np.concatenate((training_array, train_bpps_pad), axis=2)\nprivate_test_array = np.concatenate((private_test_array, private_test_bpps), axis=2)\npublic_test_array = np.concatenate((public_test_array, public_test_bpps_pad), axis=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"private_test_array = np.concatenate((private_test_array[:, :, :15], private_test_bpps), axis=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"public_test_array = np.concatenate((public_test_array[:, :, :15], public_test_bpps_pad), axis=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = torch.Tensor(public_test_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test_private = torch.Tensor(private_test_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CHANGE THE ARRAYS TO TENSORS**","metadata":{"trusted":true}},{"cell_type":"code","source":"x = torch.Tensor(training_array)\ny = torch.Tensor(training_array_targets)\nx_test_private = torch.Tensor(private_test_array)\nx_test = torch.Tensor(public_test_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CHANGING THE OUTPUTS OF THE TRAIN SET TO MATCH THE INPUT SHAPE**","metadata":{"trusted":true}},{"cell_type":"code","source":"y = y.transpose(1,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CREATE THE VALIDATION AND TRAIN SETS**","metadata":{}},{"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = get_validation_dataset(X=x, Y=y, split_ratio=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**INITIALIZE THE MODEL AND THE OPTIMIZER OBJECTS**","metadata":{"trusted":true}},{"cell_type":"code","source":"# torch.manual_seed(0)\n# np.random.seed(0)\n# torch.cuda.manual_seed(0)\n# torch.backends.cudnn.deterministic = True\n# torch.backends.cudnn.benchmark = False\n# encoder = Encoder(input_shape=15, d_model=256, nhead=4, num_layers=8, output_shape=15, dim_feedforward=512, dropout_proba=0.1, pre_train=True)\n# optimizer = Optimizer(model=encoder)                            #  4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PRE TRAIN THE OPTIMIZER**","metadata":{}},{"cell_type":"code","source":"# optimizer.fit(\n#     X=X_train,\n#     Y=X_train,\n#     X_val=X_val,\n#     Y_val=X_val,\n#     epochs=100,\n#     lr=3e-4,\n#     batch_size=8,\n#     patience=75,\n#     best_model=True,\n#     num_targets=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parameters = {key: value for key, value in dict(optimizer.model.named_parameters()).items() if not key.startswith('fully_connected')}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoder_new = Encoder(input_shape=15, d_model=256, nhead=4, num_layers=4, output_shape=5, dim_feedforward=512, dropout_proba=0.1, pre_train=False)\n# encoder_new.load_state_dict(parameters, strict=False)\n# optimizer_new = Optimizer(model=encoder_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FIT THE OPTIMIZER**","metadata":{}},{"cell_type":"code","source":"# optimizer_new.fit(X=X_train, Y=Y_train, X_val=X_val, Y_val=Y_val, epochs=150, lr=3e-4, batch_size=8, patience=75, best_model=True, num_targets=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer_new.fit(X=X_train, Y=Y_train, X_val=X_val, Y_val=Y_val, epochs=100, lr=3e-4, batch_size=16, patience=50, best_model=True, num_targets=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer_new.fit(X=X_train, Y=Y_train, X_val=X_val, Y_val=Y_val, epochs=40, lr=3e-4, batch_size=32, patience=30, best_model=True, num_targets=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer_new.fit(X=X_train, Y=Y_train, X_val=X_val, Y_val=Y_val, epochs=15, lr=3e-4, batch_size=128, patience=10, best_model=True, num_targets=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer_new.fit(X=X_train, Y=Y_train, X_val=X_val, Y_val=Y_val, epochs=15, lr=3e-4, batch_size=256, patience=10, best_model=True, num_targets=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\nnp.random.seed(0)\ntorch.cuda.manual_seed(0)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nNUM_K_FOLD = 5\nNUM_TARGETS = 5\nDROPOUT = 0.1\nPRE_TRAIN = True\nPRE_TRAIN_EPOCHS = 120\n\nfrom sklearn.model_selection import KFold\nkfold = KFold(NUM_K_FOLD, shuffle = True, random_state = 0)\n\nepochs = [200, 100, 40, 15, 15]\n# epochs = [150, 100, 40, 15, 15]\nbatch_sizes = [8, 16, 32, 128, 256]\n# batch_sizes = [8, 16, 32, 128, 256]\npatiences = [75, 50, 20, 10, 10]\n\n\nif PRE_TRAIN:\n    print(f\"- Pretraining -\")\n    encoder = Encoder(input_shape=15, d_model=256, nhead=4, num_layers=4, output_shape=15, dim_feedforward=512, dropout_proba=DROPOUT, pre_train=True)\n    optimizer = Optimizer(model=encoder)\n    optimizer.fit(\n    X=X_train,\n    Y=X_train,\n    X_val=X_val,\n    Y_val=X_val,\n    epochs=PRE_TRAIN_EPOCHS,\n    lr=3e-4,\n    batch_size=8,\n    patience=75,\n    best_model=False,\n    num_targets=15)\n    parameters = {key: value for key, value in dict(optimizer.model.named_parameters()).items() if not key.startswith('fully_connected')}\n\nmodels = []\nscores = []\nfor i, (tr_idx, va_idx) in enumerate(kfold.split(x, y)):\n    print(f\"------ Fold {i+1} start -----\")\n    \n    x_tr = x[tr_idx]\n    x_va = x[va_idx]\n    y_tr = y[tr_idx]\n    y_va = y[va_idx]\n          \n    encoder_new = Encoder(input_shape=15, d_model=256, nhead=4, num_layers=2, output_shape=5, dim_feedforward=512, dropout_proba=DROPOUT, pre_train=False)\n    if PRE_TRAIN:\n        encoder_new.load_state_dict(parameters, strict=False)\n    optimizer_new = Optimizer(model=encoder_new)\n    \n    for j in range(len(epochs)):\n        print(f\"- Finetune stage {j+1}, epochs {epochs[j]}, batch size {batch_sizes[j]}, patience {patiences[j]} -\")\n        optimizer_new.fit(X=x_tr, Y=y_tr, X_val=x_va, Y_val=y_va, epochs=epochs[j], lr=3e-4, batch_size=batch_sizes[j], patience=patiences[j], best_model=True, num_targets=NUM_TARGETS)\n    \n    models.append(optimizer_new)\n    \n    score3 = optimizer_new.evaluate(x_va, y_va, 3)\n    score5 = optimizer_new.evaluate(x_va, y_va, 5)\n    scores.append((score3, score5))\n    print(f\"----- Score 3: {score3}, Score 5: {score5} -----\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test_private.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_pub = []\npreds_private = []\nfor i, optimizer in enumerate(models):\n    preds = optimizer.predict(x_test)\n    preds = preds.transpose(1, 0).cpu().numpy()\n    preds_pub.append(preds)\n    preds = optimizer.predict(x_test_private)\n    preds = preds.transpose(1, 0).cpu().numpy()\n    preds_private.append(preds)\npreds_pub = np.array(preds_pub).mean(0)\npreds_private = np.array(preds_private).mean(0)\npreds_pub.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_private.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\", \"deg_pH10\", \"deg_50C\"]\npreds_ls = []\nfor df, preds in [(public_test_data, preds_pub), (private_test_data, preds_private)]:\n    for i, uid in enumerate(df.id):\n        single_df = pd.DataFrame(preds[i], columns=targets)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        preds_ls.append(single_df)\npreds_df = pd.concat(preds_ls)\npreds_df.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}