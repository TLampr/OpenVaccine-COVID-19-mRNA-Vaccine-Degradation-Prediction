{"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import Counter\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport torch\nimport torch.nn as nn\nfrom math import ceil\nfrom torch.utils.data import TensorDataset, RandomSampler, DataLoader\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\n\n\nclass LSTM(nn.Module):\n    def __init__(self, **kwargs):\n        super(LSTM, self).__init__()\n        self.input_shape = kwargs[\"input_shape\"]\n        self.n_output = kwargs[\"output_shape\"]\n        self.hidden_shape = kwargs[\"hidden_shape\"]\n        self.num_layers = kwargs[\"num_layers\"]\n        self.bias = kwargs[\"bias\"]\n        self.dropout = kwargs[\"dropout\"]\n        self.bidirectional = kwargs[\"bidirectional\"]\n        self.LSTM = nn.LSTM(\n            input_size=self.hidden_shape,\n            hidden_size=self.hidden_shape,\n            num_layers=self.num_layers,\n            bias=self.bias,\n            bidirectional=self.bidirectional\n        )\n        self.ReLU = nn.ReLU()\n        self.Tanh = nn.Tanh()\n        if self.bidirectional is True:\n            self.fully_connected = nn.Linear(in_features=self.hidden_shape*2, out_features=self.n_output)\n        else:\n            self.fully_connected = nn.Linear(in_features=self.hidden_shape, out_features=self.n_output)\n        self.weighted_sum = nn.Linear(in_features=2*self.hidden_shape, out_features=self.hidden_shape)\n        self.Embedding_layer = nn.Embedding(self.input_shape, self.hidden_shape)\n        self.vocabulary_indices = np.array(range(self.input_shape)).reshape(1, -1)\n\n    def forward(self, x):\n        shape = x.shape\n        x = x.numpy().astype('float')\n        x[x == 0] = np.nan\n        x_values = np.multiply(x, self.vocabulary_indices)\n        x_indices = torch.tensor(x_values[~np.isnan(x_values)].reshape(shape[0], shape[1], 2))\n        x = self.Embedding_layer(x_indices.long())\n        x = self.weighted_sum(x.reshape(shape[0], shape[1], 2*self.hidden_shape))\n        x = self.Tanh(x)\n        x = self.LSTM(x.transpose(0, 1))\n        x = self.ReLU(x[0])\n        x = self.fully_connected(x)\n        return x\n\n\nclass RMSELoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.eps = eps\n\n    def forward(self, yhat, y):\n        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n        return loss\n\n\nclass MCRMSELoss(nn.Module):\n    def __init__(self, num_scored=3):\n        super().__init__()\n        self.rmse = RMSELoss()\n        self.num_scored = num_scored\n\n    def forward(self, yhat, y):\n        score = 0\n        for i in range(self.num_scored):\n            score += self.rmse(yhat[:, :, i], y[:, :, i]) / self.num_scored\n\n        return score\n\n\nclass Our_Loss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, yhat, y):\n        error = y - yhat[:68, :, :]\n        error = torch.square(error)\n        error = torch.mean(error, dim=0)\n        error = torch.sqrt(error)\n        error = torch.mean(error)\n        return error\n\n\nclass Optimizer():\n    def __init__(self, model: torch.nn):\n        self.model = model\n\n    def get_optimizer(self, lr: float):\n        return torch.optim.Adam(self.model.parameters(), lr=lr)\n\n    def get_criterion(self):\n        return Our_Loss()\n        return MCRMSELoss()\n        return torch.nn.MSELoss()\n\n    def optimize(self, X: torch.Tensor, Y: torch.Tensor, optimizer: torch.optim.Adam):\n        optimizer.zero_grad()\n        Y_out = self.model(X)\n        criterion = self.get_criterion()\n        loss = criterion(Y_out, Y.transpose(0, 1))\n        loss.backward()\n        optimizer.step()\n        return loss.detach().item()\n\n    def evaluate(self, X, Y):\n        self.model.eval()\n        with torch.no_grad():\n            Y_out = self.model(X)\n            criterion = self.get_criterion()\n            loss = criterion(Y_out, Y.transpose(0, 1))\n        return loss.detach().item()\n\n    def get_dataloader(self, X: torch.Tensor, Y: torch.Tensor, batch_size: int) -> DataLoader:\n        dataset = TensorDataset(X, Y)\n        sampler = RandomSampler(dataset)\n        return DataLoader(dataset=dataset, sampler=sampler, batch_size=batch_size)\n\n    def get_validation_dataset(self, X: torch.Tensor, Y: torch.Tensor, split_ratio: float):\n        X, Y = shuffle(X, Y)\n        X_train, X_val = X[ceil(split_ratio*len(X)):], X[:ceil(split_ratio*len(X))]\n        Y_train, Y_val = Y[ceil(split_ratio * len(Y)):], Y[:ceil(split_ratio * len(Y))]\n        return X_train, X_val, Y_train, Y_val\n\n    def fit(self, X: torch.Tensor, Y: torch.Tensor, epochs: int, lr: float, batch_size: int):\n        X_train, X_val, Y_train, Y_val = self.get_validation_dataset(X=X, Y=Y, split_ratio=0.2)\n        train_dataset = self.get_dataloader(X=X_train[:1], Y=Y_train[:1], batch_size=batch_size)\n        val_dataset = self.get_dataloader(X=X_val, Y=Y_val, batch_size=batch_size)\n        optimizer = self.get_optimizer(lr=lr)\n        train_loss_history, val_loss_history = [], []\n        for epoch in range(epochs):\n            train_batch_loss_history, val_batch_loss_history = [], []\n            for step_num, [X_batch, Y_batch] in enumerate(train_dataset):\n                step_loss = self.optimize(X=X_batch, Y=Y_batch, optimizer=optimizer)\n                train_batch_loss_history.append(step_loss)\n            train_loss_history.append(np.average(train_batch_loss_history))\n            for step_num, [X_batch, Y_batch] in enumerate(val_dataset):\n                val_step_loss = self.evaluate(X=X_batch, Y=Y_batch)\n                val_batch_loss_history.append(val_step_loss)\n            val_loss_history.append(np.average(val_batch_loss_history))\n            print(f\"train loss: {train_batch_loss_history[-1]} \\tvalidation loss: {val_batch_loss_history[-1]} for epoch {epoch + 1}\")\n            \n\nif __name__ == '__main__':\n\n    train_data = pd.read_json('../input/stanford-covid-vaccine/train.json',lines=True)\n    test_data = pd.read_json('../input/stanford-covid-vaccine/test.json',lines=True)\n    public_test_data = test_data[test_data['seq_length'] == 107]\n\n    def pair_indices(structure, sequence):\n        stack_base = []\n        stack_index = []\n        seqpairposmap = []\n        seqpairposmapbase = []\n        seqpairpos_dist = []\n        for i in range(len(structure)):\n            if structure[i] == '(':\n                stack_base.append(sequence[i])\n                stack_index.append(i)\n            if structure[i] == ')':\n                pairpos = stack_index.pop()\n                pairpos_base = stack_base.pop()\n                seqpairposmap.append((i, pairpos))\n                pair = (sequence[i], pairpos_base)\n                pair = tuple(sorted(pair))\n                seqpairposmapbase.append(pair)\n        return seqpairposmapbase\n\n    public_test_data['pairs'] = public_test_data.apply(lambda x: pair_indices(x.structure, x.sequence), axis=1)\n    public_test_data['pairs'] = public_test_data['pairs'].apply(lambda x: Counter(x))\n    pd.concat([public_test_data.drop(['pairs'], axis=1), public_test_data['pairs'].apply(pd.Series)], axis=1)\n\n    sample = train_data['sequence'][0]\n    sample = [char for char in sample]\n    encoder = OneHotEncoder()\n    encoder.fit(np.array(sample).reshape(-1,1))\n    train_data['sequence_encoding'] = train_data['sequence'].apply(lambda x: [char for char in x])\n    train_data['sequence_encoding']=train_data['sequence_encoding'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())\n    encoder_data = train_data['predicted_loop_type'][0] + train_data['predicted_loop_type'][1] + train_data['predicted_loop_type'][2]\n    encoder_data = [char for char in encoder_data]\n    encoder_data = np.array(encoder_data).reshape(-1,1)\n    encoder.fit(encoder_data)\n    train_data['predicted_loop_type_encoding'] = train_data['predicted_loop_type'].apply(lambda x: [char for char in x])\n    train_data['predicted_loop_type_encoding'] = train_data['predicted_loop_type_encoding'].apply(lambda x: encoder.transform(np.array(x).reshape(-1,1)).todense())\n    training_array = [np.concatenate(train_data[['sequence_encoding', 'predicted_loop_type_encoding']].to_numpy()[i], axis=1) for i in range(train_data.shape[0])]\n    training_array = np.array(training_array)\n    training_array_targets = [np.concatenate((np.array(train_data['reactivity'][i]).reshape(1,-1), np.array(train_data['deg_Mg_pH10'][i]).reshape(1,-1), np.array(train_data['deg_pH10'][i]).reshape(1,-1), np.array(train_data['deg_Mg_50C'][i]).reshape(1,-1), np.array(train_data['deg_50C'][i]).reshape(1,-1)), axis=0) for i in range(train_data.shape[0])]\n    training_array_targets = np.array(training_array_targets)\n    x = torch.Tensor(training_array)\n    y = torch.Tensor(training_array_targets)\n    y = y.transpose(1,2)\n    lstm = LSTM(input_shape=11, hidden_shape=248, num_layers=12, bias=True, dropout=0.0, bidirectional=False, output_shape=5)\n    optimizer = Optimizer(model=lstm)\n    optimizer.fit(X=x, Y=y, epochs=100, lr=3e-4, batch_size=48)\n\n","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]}]}